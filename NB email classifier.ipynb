{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c5f1ce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "                                            wordList  classList\n",
      "0  codeine,15mg,for,203,visa,only,codeine,methylm...          1\n",
      "1  peter,with,jose,out,town,you,want,meet,once,wh...          0\n",
      "2  hydrocodone,vicodin,brand,watson,vicodin,750,1...          1\n",
      "3  yay,you,both,doing,fine,working,mba,design,str...          0\n",
      "4  you,have,everything,gain,incredib1e,gains,leng...          1\n",
      "   classList\n",
      "0          1\n",
      "1          0\n",
      "2          1\n",
      "3          0\n",
      "4          1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import scipy as sp\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Reading the dataset\n",
    "dfw = pd.read_csv('wordsList', sep=' ', header=None)\n",
    "dfw.columns = ['wordList']\n",
    "\n",
    "dfc = pd.read_csv('classList', sep=\" \", header=None)\n",
    "dfc.columns = ['classList']\n",
    "\n",
    "# add classList column to dfw\n",
    "dfw['classList'] = dfc['classList']\n",
    "print(dfw.shape[0])\n",
    "\n",
    "# To print the first 5 rows of wordsList\n",
    "print(dfw.head())\n",
    "\n",
    "# to print the first 5 rows of classList\n",
    "print(dfc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c3eeee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 2)\n",
      "1    37\n",
      "0    35\n",
      "Name: classList, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dfw.shape)\n",
    "print(dfw.classList.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b386d756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training no of rows X: 66\n",
      "Testing no of rows X: 6\n",
      "Training no of rows y: 66\n",
      "Testing no of rows y: 6\n"
     ]
    }
   ],
   "source": [
    "#Stratified sampling as requested from the question\n",
    "import random\n",
    "def train_test_split(X, y, test_size):\n",
    "    \n",
    "    if isinstance(test_size, float):\n",
    "        test_size = round(test_size * len(X))\n",
    "\n",
    "    indices = X.index.tolist()\n",
    "    test_indices = random.sample(population=indices, k=test_size)\n",
    "\n",
    "    test_X = X.loc[test_indices]\n",
    "    train_X = X.drop(test_indices)\n",
    "    \n",
    "    if isinstance(test_size, float):\n",
    "        test_size = round(test_size * len(y))\n",
    "\n",
    "    indices = y.index.tolist()\n",
    "    test_indices = random.sample(population=indices, k=test_size)\n",
    "    \n",
    "    test_y = y.loc[test_indices]\n",
    "    train_y = y.drop(test_indices)\n",
    "    \n",
    "    return train_X, test_X, train_y, test_y\n",
    "\n",
    "X = dfw.wordList\n",
    "y = dfc.classList\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.08)\n",
    "print(\"Training no of rows X: \"+str(len(X_train)))\n",
    "print(\"Testing no of rows X: \"+str(len(X_test)))\n",
    "print(\"Training no of rows y: \"+str(len(y_train)))\n",
    "print(\"Testing no of rows y: \"+str(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "731017f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(str_arg):    \n",
    "    #cleaned_str=re.sub('[^a-z\\s]+',' ',str_arg,flags=re.IGNORECASE) \n",
    "    #every char except alphabets is replaced\n",
    "    #cleaned_str=re.sub('(\\s+)',' ',cleaned_str) \n",
    "    #multiple spaces are replaced by single space\n",
    "    cleaned_str=str_arg.lower() \n",
    "    #converting the cleaned string to lower case\n",
    "    \n",
    "    return cleaned_str # returning the preprocessed string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2741f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self,unique_classes):\n",
    "        \n",
    "        self.classes=unique_classes\n",
    "        # Constructor is simply passed with unique number of classes of the training set\n",
    "        \n",
    "\n",
    "    def addToBow(self,example,dict_index):\n",
    "        \n",
    "        if isinstance(example,np.ndarray): example=example[0]\n",
    "     \n",
    "        for token_word in example.split(\",\"): \n",
    "            #for every word in preprocessed example\n",
    "          \n",
    "            self.bow_dicts[dict_index][token_word]+=1 #increment in its count\n",
    "            \n",
    "    def train(self,dataset,labels):\n",
    "    \n",
    "        self.examples=dataset\n",
    "        self.labels=labels\n",
    "        self.bow_dicts=np.array([defaultdict(lambda:0) for index in range(self.classes.shape[0])])\n",
    "        \n",
    "        #only convert to numpy arrays if initially not passed as numpy arrays - else its a useless recomputation\n",
    "        \n",
    "        if not isinstance(self.examples,np.ndarray): self.examples=np.array(self.examples)\n",
    "        if not isinstance(self.labels,np.ndarray): self.labels=np.array(self.labels)\n",
    "            \n",
    "        #constructing BoW for each category\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "          \n",
    "            all_cat_examples=self.examples[self.labels==cat]\n",
    "            #filter all examples of category == cat\n",
    "            \n",
    "            #get examples preprocessed\n",
    "            \n",
    "            cleaned_examples=[preprocess_string(cat_example) for cat_example in all_cat_examples]\n",
    "            \n",
    "            cleaned_examples=pd.DataFrame(data=cleaned_examples)\n",
    "            \n",
    "            #now costruct BoW of this particular category\n",
    "            np.apply_along_axis(self.addToBow,1,cleaned_examples,cat_index)\n",
    "\n",
    "        prob_classes=np.empty(self.classes.shape[0])\n",
    "        all_words=[]\n",
    "        cat_word_counts=np.empty(self.classes.shape[0])\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "           \n",
    "            #Calculating prior probability p(c) for each class\n",
    "            prob_classes[cat_index]=np.sum(self.labels==cat)/float(self.labels.shape[0]) \n",
    "            \n",
    "            #Calculating total counts of all the words of each class \n",
    "            count=list(self.bow_dicts[cat_index].values())\n",
    "            cat_word_counts[cat_index]=np.sum(np.array(list(self.bow_dicts[cat_index].values())))+1 # |v| is remaining to be added\n",
    "            \n",
    "            #get all words of this category                                \n",
    "            all_words+=self.bow_dicts[cat_index].keys()\n",
    "                                                     \n",
    "        #combine all words of every category & make them unique to get vocabulary -V- of entire training set\n",
    "        \n",
    "        self.vocab=np.unique(np.array(all_words))\n",
    "        self.vocab_length=self.vocab.shape[0]\n",
    "                                  \n",
    "        #computing denominator value                                      \n",
    "        denoms=np.array([cat_word_counts[cat_index]+self.vocab_length+1 for cat_index,cat in enumerate(self.classes)])                                                                          \n",
    "\n",
    "        self.cats_info=[(self.bow_dicts[cat_index],prob_classes[cat_index],denoms[cat_index]) for cat_index,cat in enumerate(self.classes)]                               \n",
    "        self.cats_info=np.array(self.cats_info)                                 \n",
    "                                              \n",
    "                                              \n",
    "    def getExampleProb(self,test_example):                                \n",
    "\n",
    "        likelihood_prob=np.zeros(self.classes.shape[0]) #to store probability w.r.t each class\n",
    "        \n",
    "        #finding probability w.r.t each class of the given test example\n",
    "        for cat_index,cat in enumerate(self.classes): \n",
    "                             \n",
    "            for test_token in test_example.split(): #split the test example and get p of each test word                       \n",
    "                #This loop computes : for each word w [ count(w|c)+1 ] / [ count(c) + |V| + 1 ]                               \n",
    "\n",
    "                #get total count of this test token from it's respective training dict to get numerator value                           \n",
    "                test_token_counts=self.cats_info[cat_index][0].get(test_token,0)+1\n",
    "                \n",
    "                #now get likelihood of this test_token word                              \n",
    "                test_token_prob=test_token_counts/float(self.cats_info[cat_index][2])                              \n",
    "                \n",
    "                #remember why taking log? To prevent underflow!\n",
    "                likelihood_prob[cat_index]+=np.log(test_token_prob)\n",
    "                                              \n",
    "        # we have likelihood estimate of the given example against every class but we need posterior probility\n",
    "        post_prob=np.empty(self.classes.shape[0])\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "            post_prob[cat_index]=likelihood_prob[cat_index]+np.log(self.cats_info[cat_index][1])                                  \n",
    "      \n",
    "        return post_prob\n",
    "    \n",
    "   \n",
    "    def test(self,test_set):\n",
    "\n",
    "        predictions=[] #to store prediction of each test example\n",
    "        for example in test_set: \n",
    "                                              \n",
    "            #preprocess the test example the same way we did for training set exampels                                  \n",
    "            cleaned_example=preprocess_string(example) \n",
    "             \n",
    "            #simply get the posterior probability of every example                                  \n",
    "            post_prob=self.getExampleProb(cleaned_example) #get prob of this example for both classes\n",
    "            \n",
    "            #simply pick the max value and map against self.classes!\n",
    "            predictions.append(self.classes[np.argmax(post_prob)])\n",
    "                \n",
    "        return np.array(predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "569ddedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.NaiveBayes at 0x1fa9028ff40>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb=NaiveBayes(np.unique(y_train))\n",
    "nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "009f32a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb.train(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1430b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Examples:  6\n",
      "Test Set Accuracy:  66.66666666666666 %\n"
     ]
    }
   ],
   "source": [
    "pclasses=nb.test(X_test) #get predcitions for test set\n",
    "\n",
    "#check how many predcitions actually match original test labels\n",
    "test_acc=np.sum(pclasses==y_test)/float(y_test.shape[0]) \n",
    "\n",
    "print (\"Test Set Examples: \",y_test.shape[0])\n",
    "print (\"Test Set Accuracy: \",test_acc*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447cf6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
